# 并行思维与编程
《计算机体系结构——量化方法》
学习体系结构可能的关键点：
* 
![test-img](../screen_shot/Screenshot_20230720_022212_WPS%20Office.jpg)

## 共享内存的硬件
首先要了解一些必要的组成原理知识，其中两本书比较有作用
* 计算机体系结构-量化方法

### Instruction-Level parallelism
一个指令流中多个指令同时执行
#### 流水线
#### 超标量
#### 超长指令字
#### 乱序执行

### Data-Level Parallelism
主要涉及到的内容有向量运算，SIMD，GPU架构
#### 向量架构
向量概念,向量由一组有序，相同类型和位数的元素组成


#### 指令集扩展
#### 脉动阵列，TPU
#### GPUs
#### 设计并行化的数据结构

### Thread-Level Parallelism
多核架构的火热起因：
* 硅成本和功耗的增长速度高于性能提升，效率降低
* 云计算时代，大数据，数据密集型应用的大规模应用
* 服务器环境大多是多核的



### Cache Coherence
### MSI Protocol
首先要了解一个重要的概念：TLP（thread-level parallel）

## 组成原理&汇编语言

### C2ASM


## 并发原语
### pthread
### Intel-Threading Building Blocks
TBB的操纵对象是Task，TBB框架的关键特性包括：
* 调度对象为task
* parallel_for/parallel_reduce/pipeline等C++模板等用作数据的常见操纵模式
* 提供了许多并发容器类（concurrent container），允许多线程安全地访问和更新容器

### OpenMP
### Cilk
Cilk是C／C++的linguistic extensions，支持fork-join并行
* Cilk Plus由Cilk Arts进行开发
* 该工具由Cilk改进而来，Cilk是由MIT提出的
* 知名调度器:work-stealing scheduler
* 要用这个工具，最好配合Tapir/LLVM，the best Cilk compiler
* 执行的时候到底要不要唤醒多核来运行，取决于Cilk的runtime system

一段并行的Cilk代码展示
```c++
int64_t fib(int64_t n) {
    if (n < 2) {
        return n;
    } else {
        int64_t x, y;
        x = cilk_spawn fib(n-1);
        y = fib(n-2);
        cilk_sync;
        return(x + y);
    }
}
```

矩阵转置
```c++
cilk_for (int i=1; i<n; ++i) {
    for (int j=0; j<i; ++j) {
        double temp = A[i][j];
        A[i][j] = A[j][i];
        A[j][i] = temp;
    }
}
```

来看一段代码，考虑这段代码是否奏效
```c++
unsigned long sum = 0;
for (int i=0; i<n; ++i) {
    sum += i;
}
printf("%d\n", sum);
```
代码中多个线程可能会访问同一段内存，这会触发determinacy race，这里需要引入一个新的概念：cilk的**hyperobject**。在这个例子中，我们引入**Reducer**。将代码改写。
```c++
CILK_C_REDUCER_OPADD(sum, unsigned long, 0);
CILK_C_REGISTER_REDUCER(sum);
cilk_for (int i=0; i<n; ++i) {
    REDUCER_VIEW(sum) += i;
}
CILK_C_UNREGISTER_REDUCER(sum);
```

## CUDA/加速卡的并行编程


## Python的多进程并行

### 前提准备：Pytest的学习

@pytest.fixture

test_function



### pandas的并行

读pandarallel的核心代码

* linux下多进程并行的一种实践
* 学习pandarallel并掌握其原理
* pandas多进程并行的编程实践
* 关于效率的进一步思考



先看实验

```python
import time
import pandas as pd
import numpy as np
from pandarallel.pandarallel import pandarallel

# 在linux/wsl上进行实验，保证能够使用memory file system
def static(func):
    start = time.time()
    func()
    end = time.time()
    print(f"spent {end - start} on computing")
    
pandarallel.initialize(progress_bar=True)
"""
output:
INFO: Pandarallel will run on 6 workers.
INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.
"""

# 生成实验数据，简单写个benchmark
df = pd.DataFrame(np.random.binomial(n=500000, p=0.2, size=(10000, 2)))
def func(x):
    # 注意在windows下测试的话，import math一定要在function的scope里面，保证
    # spawn能够成功
    import math
    return math.sin(x[0]**2) + math.sin(x[1]**2)

def test_serial():
    df.apply(func, axis=1)
    
def test_parallel():
    df.parallel_apply(func, axis=1)
    
static(test_serial)
"""
spent 4.144069194793701 on computing
"""

static(test_parallel)
"""
spent 1.4205896854400635 on computing
"""
```



#### 核心API分析

```python
# Unix环境下，使用/dev/shm作为内存的文件系统
def parallelize_with_memory_file_system(
    nb_requested_workers: int,
    data_type: Type[DataType],
    progress_bars_type: ProgressBarsType,
):
    def closure(
        data: Any,
        user_defined_function: Callable,
        *user_defined_function_args: tuple,
        **user_defined_function_kwargs: Dict[str, Any],
    ):
        wrapped_work_function = WrapWorkFunctionForFileSystem(data_type.work)
        wrapped_reduce_function = wrap_reduce_function_for_file_system(data_type.reduce)

        chunks = list(
            data_type.get_chunks(
                nb_requested_workers,
                data,
                user_defined_function_kwargs=user_defined_function_kwargs,
            )
        )

        nb_workers = len(chunks)

        multiplicator_factor = (
            len(cast(pd.DataFrame, data).columns)
            if progress_bars_type
            == ProgressBarsType.InUserDefinedFunctionMultiplyByNumberOfColumns
            else 1
        )

        progresses_length = [len(chunk_) * multiplicator_factor for chunk_ in chunks]

        work_extra = data_type.get_work_extra(data)
        reduce_extra = data_type.get_reduce_extra(data, user_defined_function_kwargs)

        show_progress_bars = progress_bars_type != ProgressBarsType.No

        progress_bars = get_progress_bars(progresses_length, show_progress_bars)
        progresses = [0] * nb_workers
        workers_status = [WorkerStatus.Running] * nb_workers

        input_files = [
            NamedTemporaryFile(
                prefix=PREFIX_INPUT, suffix=SUFFIX, dir=MEMORY_FS_ROOT, delete=False
            )
            for _ in range(nb_workers)
        ]

        output_files = [
            NamedTemporaryFile(
                prefix=PREFIX_OUTPUT, suffix=SUFFIX, dir=MEMORY_FS_ROOT, delete=False
            )
            for _ in range(nb_workers)
        ]

        try:
            for chunk, input_file in zip(chunks, input_files):
                with Path(input_file.name).open("wb") as file_descriptor:
                    pickle.dump(chunk, file_descriptor)

            dilled_user_defined_function = dill.dumps(user_defined_function)
            manager: SyncManager = CONTEXT.Manager()
            master_workers_queue = manager.Queue()

            work_args_list = [
                (
                    Path(input_file.name),
                    Path(output_file.name),
                    progress_bars_type,
                    worker_index,
                    master_workers_queue,
                    dilled_user_defined_function,
                    user_defined_function_args,
                    user_defined_function_kwargs,
                    {
                        **work_extra,
                        **{
                            "master_workers_queue": master_workers_queue,
                            "show_progress_bars": show_progress_bars,
                            "worker_index": worker_index,
                        },
                    },
                )
                for worker_index, (
                    input_file,
                    output_file,
                ) in enumerate(zip(input_files, output_files))
            ]

            pool = CONTEXT.Pool(nb_workers)
            pool.starmap_async(wrapped_work_function, work_args_list)

            pool.close()

            generation = count()

            while any(
                (
                    worker_status == WorkerStatus.Running
                    for worker_status in workers_status
                )
            ):
                message: Tuple[int, WorkerStatus, Any] = master_workers_queue.get()
                worker_index, worker_status, payload = message
                workers_status[worker_index] = worker_status

                if worker_status == WorkerStatus.Success:
                    progresses[worker_index] = progresses_length[worker_index]
                    progress_bars.update(progresses)
                elif worker_status == WorkerStatus.Running:
                    progress = cast(int, payload)
                    progresses[worker_index] = progress

                    if next(generation) % nb_workers == 0:
                        progress_bars.update(progresses)
                elif worker_status == WorkerStatus.Error:
                    progress_bars.set_error(worker_index)
                    progress_bars.update(progresses)

            return wrapped_reduce_function(
                (Path(output_file.name) for output_file in output_files),
                reduce_extra,
            )

        finally:
            for output_file in output_files:
                # When pandarallel stop supporting Python 3.7 and older, replace this
                # try/except clause by:
                # Path(output_file.name).unlink(missing_ok=True)
                try:
                    Path(output_file.name).unlink()
                except FileNotFoundError:
                    # Do nothing, this is the nominal case.
                    pass

    return closure
```



#### Memory file system

* 关于/dev/shm

`/dev/shm` is a temporary file storage filesystem, i.e., [tmpfs](http://en.wikipedia.org/wiki/TMPFS), that uses RAM for the backing store. It can function as a shared memory implementation that facilitates [IPC](http://en.wikipedia.org/wiki/Inter-process_communication).

参照[这个链接](https://superuser.com/questions/45342/when-should-i-use-dev-shm-and-when-should-i-use-tmp)



#### multiprocessing pool

* starmap_async(wrapped_work_function, work_args_list)是对worker下发指令的地方





#### 实现的细节

> 如何wrap user_defined_function？

mfs实现并行的最核心逻辑在WrapWorkFunctionForFileSystem的`__call__`中（core.py的57行）：

外层API将df分块后dump进了每个worker对应的文件中后，就可以进行计算了，每个worker所需要的参数包括:

```python
input_file_path: Path,
output_file_path: Path,
progress_bars_type: ProgressBarsType,
worker_index: int,
master_workers_queue: multiprocessing.Queue,
dilled_user_defined_function: bytes,
user_defined_function_args: tuple,
user_defined_function_kwargs: Dict[str, Any],
extra: Dict[str, Any]
```

首先要理解此时，每个worker都是一个单独的解释器

每个worker所做的事，可被分解为：

* 从input_file中读取数据
* unlink：即删除该文件，释放掉多余的内存占用
* user_function反序列化
* 调用每个数据类型的work方法（比如dataframe.apply）
* 将计算的结果dump到output_file中
* 将整个过程是否运行成功作为信号传给master_workers_queue，成功传入WorkerStatus.Success，失败则传入WorkerStatus.Error



> dump

* 数据的序列化使用的是pickle.dump
* 函数的序列化使用的是dill.dumps



> reduce vs work

work和reduce都是上述过程中被wrapped的内容

其中work对应的是data_type被分解为chunks之后，每个chunk进行计算的接口，比如dataframe的apply

而reduce是datatype计算完成后，将结果整合成一个输出的调用



> chunks

对范围分割的逻辑单独被封装为chunk方法，关键是对于不同的data_type，其作用的分块对象不一致，因此需要每个data_type实现自己的get_chunks方法

其中一个很重要的问题就是：**axis该怎么选**



#### 一些问题

* 使用pandas，为何不要遍历？
* pandas性能上的最佳实践？



## Python的多线程

### edgedb的实践





### Why GIL

GIL是广受Python程序员诟病的一个运行时限制，它是程序员们无法使用Python多线程绑多核编程的原因。

但如果能直接剔除掉GIL，开发者们早就去做了，之所以没这么做，是因为GIL保证了Python的线程安全



下面举例：



### Why remove the GIL

要看原文：https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit#

那GIL真的不能被掰开吗？

**虽然很难啊，但只要努力，就能办到**，nogil的开发者们说道

为啥要掰开它？

* 它是广义Concurrency的障碍。对于科学计算来说，缺少并发能力，比单个解释器执行效率低导致的后果更加严重。
* 在 “Why Swift for Tensorflow”的文档中，Chris Lattner说，为了达到预期的性能水平，程序员们必须学很多，比如学CPP和CUDA，无法在编程中自由地写出并发算法；
* Python多进程编程不自由，体现在worker之间的通信代价很大，worker的初始化效率相较线程低太多；通信代价大这个可以举个例子，Pytorch的Dataloader，有时候数据搬迁的实践比跑一次forward pass还要大
* 写C/CPP扩展在很多场景确实有用，不过对开发者要求很高，而且要写很多，比如pytorch把自动微分的代码全用CPP重新实现，就是因为py不能并行



### How to remove

* 如前文所提到，引用计数在没有GIL的情况下是非线程安全的，最简单的解决方法就是把涉及到的操作从“非原子”的转换到“原子的”，但这么做代价很大，将Py_INCREF和Py_DECREF替换成原子变量会导致60%的平均性能下降（基于pyperformance benchmark suite）
* 咋搞？从算法角度搞，有个大佬提出了“带偏见的引用计数”这个解决方案，简单来说，就是每个对象都和其创建线程绑定，创建线程使用非原子指令来修改"local"计数器，其他线程则使用"shared"计数器，具体看这https://dl.acm.org/doi/10.1145/3243176.3243195；那显然这种方法的话，本地线程对本地变量的访问是高效的，那。。另外一种访问呢？
* nogil用到了两种技术来提高非本地的变量访问：**永生**与**延时引用**
* 永生解释起来比较简单，有一些对象，比如内置的字符串，小整形，静态分配的PyTypeObjects以及True, False,None这些对象在整个解释器的生命周期内都是活跃的，他们会被标记为“永生”（咋标记？通过local引用计数对应的最低有效比特位），所以Py_INCREF以及Py_DECREF宏对这些对象是无法生效的
* 延时引用：诸如最高层的函数，code objects以及模块对象会被多个线程频繁地并发访问，但并不是整个生命周期内都是必须的，所以对他们使用永生不太合适，



## Python asyncio

```python
import asyncio
import time
global_flag = True


async def sub_task():
    await asyncio.sleep(5)
    print("task finished!!")


async def inner_loop():
    global global_flag
    while global_flag:
        print("in loop!")
        asyncio.create_task(sub_task())
        await asyncio.sleep(3)


async def long_duration():
    global global_flag
    await asyncio.sleep(20)
    global_flag = False


async def test_call():
    start_time = time.time()
    await asyncio.wait([
        asyncio.create_task(long_duration()),
        asyncio.create_task(inner_loop())
    ])
    end_time = time.time()
    print(f"cost time {end_time-start_time}")


loop = asyncio.get_event_loop()
loop.run_until_complete(test_call())
```

### async的重要概念

* event_loop
* 长时间的IO
  * 磁盘读写
  * 网络请求
* await
* 协程本身



### 和线程的关系

* 协程存在于单个线程中

* 协程本质上是分割单个线程的时间片
* 非阻塞和协程







## MPI4PY

