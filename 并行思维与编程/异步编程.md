# 异步编程

你会学习到：

* epoll/kqueue等事件队列/多路复用机制
  
  * 原理，好处
  
  * 使用&抽象方法（rust编码实例）
  
  * mio的抽象方法（着重于关键抽象）
  
  * 为stackless协程铺垫

* stackful协程

## 异步模型

* 协作方式
  
  * 抢占式
  
  * 非抢占式

* 是否使用栈空间

<br>

## 操作系统层面

本节会介绍操作系统层面的知识：

* 固件

* 事件队列

### 固件

简单来说，固件就像是**嵌入在硬件设备内部的“迷你”软件**，它直接控制着该硬件的基本操作和功能。它不像操作系统或应用程序那样安装在硬盘上，而是通常固化在硬件的只读存储器（ROM）或闪存中。

**固件在异步编程中的作用和工作方式：**

1. **硬件的直接控制者：**
   
   - 像网卡、硬盘控制器、显卡（的BIOS部分）、主板（BIOS/UEFI）等计算机部件都有自己的固件。这些固件负责最底层的操作，比如网卡固件知道如何接收和发送网络数据包，硬盘固件知道如何读取和写入磁盘扇区。
   
   - 很多时候，这些硬件设备本身就带有一个小型的、专用的处理器（微控制器），固件就运行在这个微控制器上。这很重要，因为它意味着硬件设备可以在**不占用主CPU**的情况下独立完成一些工作。（书中 P22 提到）

2. **事件检测与发起通知：**
   
   - 这是固件在异步I/O中最关键的作用。当一个外部事件发生时（例如，网卡接收到数据包，硬盘完成了数据读取），是**运行在设备微控制器上的固件**首先检测到这个状态变化。
   
   - 固件检测到事件后，它并**不是**直接通知你的应用程序。相反，它会采取一种更高效的方式来通知系统的其他部分。

3. **触发中断（Interrupts）：**
   
   - 固件检测到关键事件（如数据准备好、操作完成）后，它会**指示硬件**向主CPU发送一个**硬件中断信号**（一个电信号，通过中断请求线IRQ发送）。（书中 P20-P21 描述了这个流程）
   
   - 这个中断信号会**打断**CPU当前正在执行的任务，强制CPU去处理这个中断。CPU会根据中断信号找到操作系统预设好的**中断处理程序**（Interrupt Handler，通常是设备驱动程序的一部分）。

4. **配合DMA（直接内存访问）：**
   
   - 为了进一步提高效率，很多现代硬件（如网卡、硬盘）的固件会配合DMA控制器工作。当数据需要传输时（比如从网卡接收数据到内存），固件会设置好DMA控制器，让数据直接在设备和主内存之间传输，**绕过CPU**，CPU无需介入每一个字节的传输。
   
   - 传输完成后，固件（或DMA控制器）才会触发一个中断，通知CPU“数据已经准备好了”。

5. **实现异步的关键——“通知”而非“轮询”：**
   
   - 想象一下，如果没有固件和中断机制，操作系统或应用程序想要知道网卡有没有收到数据，就只能不断地去**轮询**（Polling）网卡：“数据来了吗？数据来了吗？”。这会极大地浪费CPU资源，因为CPU大部分时间都在做无意义的查询。
   
   - 固件的角色就是让设备具备了**主动通知**的能力。固件在设备内部可能自己进行某种形式的“轮询”或等待，但这发生在设备自己的微控制器上。一旦有事发生，它通过中断机制“**通知**”操作系统。
   
   - 操作系统收到了这个硬件中断（由固件发起），就知道某个I/O操作有了进展（比如数据可读了）。然后操作系统会查找是哪个应用程序的哪个任务在等待这个事件，并**唤醒**这个等待的任务（在Rust的异步模型中，就是调用对应的Waker），让异步运行时（Executor）可以重新调度执行这个任务。（书中 P22 强调了“通知”优于“轮询”的效率优势）

**总结来说：**

在异步编程的场景下，固件是硬件设备的“大脑”，负责：

- **直接操作硬件**完成底层任务（收发数据、读写磁盘等）。

- **检测**硬件状态的变化和外部事件的发生。

- 通过**硬件中断**机制，**高效地**将事件的发生**通知**给操作系统和CPU，而不是让CPU或操作系统浪费资源去反复轮询。

- （可选）配合**DMA**控制器，在**不占用主CPU**的情况下完成数据传输。

正是因为有了固件及其配合的中断、DMA等机制，硬件才能在完成任务后主动通知系统，操作系统才能据此唤醒等待的异步任务，从而实现了**非阻塞I/O**，让CPU可以在等待I/O期间去执行其他计算任务，这就是异步编程提高效率的基础。没有固件的这种底层工作机制，高效的异步编程几乎是不可能实现的。

<br>

### 事件队列

事件队列是操作系统用于**管理异步事件**的数据结构，其核心功能是：

- **事件存储**：记录来自硬件（如网卡中断、磁盘I/O完成）或软件（如定时器、进程间通信）的事件通知。
- **事件调度**：按优先级或时间顺序将事件传递给应用程序，例如通过回调函数或唤醒阻塞线程[1](https://baike.baidu.com/item/%E4%BA%8B%E4%BB%B6%E9%98%9F%E5%88%97/0)，[4](https://www.cnblogs.com/fqlGlog/articles/7536449.html)，[7](https://gpp.tkchu.me/event-queue.html)。
- **解耦生产与消费**：事件的产生（如网络数据到达）与处理（如应用程序读取数据）通过队列分离，避免阻塞[7](https://gpp.tkchu.me/event-queue.html)。

<br>

**典型应用场景**：

- **GUI事件处理**：用户点击、键盘输入等事件通过队列传递给应用程序[7](https://gpp.tkchu.me/event-queue.html)。
- **异步I/O**：如网络通信中，网卡接收数据后触发中断，内核将事件加入队列，通知应用程序读取[1](https://baike.baidu.com/item/%E4%BA%8B%E4%BB%B6%E9%98%9F%E5%88%97/0)，[7](https://gpp.tkchu.me/event-queue.html)。
- **定时任务**：定时器到期事件通过队列调度[4](https://www.cnblogs.com/fqlGlog/articles/7536449.html)，[6](https://baijiahao.baidu.com/s?id=1685017463288229440)。

### epoll/kqueue/IOCP

| **机制**     | **操作系统**  | **核心原理**                            | **与事件队列的关联**                                                                                                                                         |
| ---------- | --------- | ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| **epoll**  | Linux     | 基于**红黑树+就绪链表**，仅返回活跃的文件描述符（FD）。     | 内核维护一个**I/O事件队列**，epoll通过`epoll_wait`从队列中获取就绪事件[4](https://www.cnblogs.com/fqlGlog/articles/7536449.html)[7](https://gpp.tkchu.me/event-queue.html)。 |
| **kqueue** | BSD/macOS | 支持多种事件类型（文件、信号、定时器等），通过**事件过滤器**管理。 | 内核将事件分类存储到**多个队列**，kqueue通过`kevent`统一监听和提取事件[4](https://www.cnblogs.com/fqlGlog/articles/7536449.html)，[7](https://gpp.tkchu.me/event-queue.html)。   |
| **IOCP**   | Windows   | 基于**完成端口模型**，与线程池结合实现异步I/O。         | 内核将I/O操作结果直接写入**完成队列**，应用程序通过`GetQueuedCompletionStatus`轮询队列[7](https://gpp.tkchu.me/event-queue.html)。                                              |

以网络服务器接收连接为例，结合事件队列和epoll的工作流程：

1. **注册事件**：应用程序通过`epoll_ctl`将Socket FD注册到epoll实例，内核将其加入**监听队列**。
2. **事件触发**：网卡收到数据后触发中断，内核将FD标记为就绪，移入**就绪队列**。
3. **事件获取**：`epoll_wait`从就绪队列中取出事件，返回给应用程序。
4. **事件处理**：应用程序根据事件类型（如可读、可写）执行回调或线程任务[4](https://www.cnblogs.com/fqlGlog/articles/7536449.html)，[7](https://gpp.tkchu.me/event-queue.html)。

这一过程通过内核与用户态协作，**事件队列作为中间层**，实现了高效的资源调度。

<br>

#### readiness-based event queue

epoll和kqueue是基于准备状态的事件队列，我们从一个socket应用出发，构建一个工作流：

* 通过epoll_create创建一个事件队列

* 获取socket的fd

* 通过系统调用注册读取事件

* 调用epoll_wait或者kevent来等待事件，这将会使得他被调用的线程阻塞

* 当事件被触发了，线程将会被释放

那么问题来了，异步是如何实现的呢？

一个案例，使用epoll实现TCP监听服务器

```c
// 创建 epoll 实例
int epfd = epoll_create1(0);

// 监听 socket 初始化（略）
int listen_sock = socket(...);
bind(...);
listen(...);

// 注册监听 socket 到 epoll
struct epoll_event ev;
ev.events = EPOLLIN | EPOLLET;  // 监听可读事件（边缘触发）
ev.data.fd = listen_sock;
epoll_ctl(epfd, EPOLL_CTL_ADD, listen_sock, &ev);

// 事件循环
struct epoll_event events[MAX_EVENTS];
while (1) {
    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
    for (int i = 0; i < nfds; i++) {
        if (events[i].data.fd == listen_sock) {
            // 接受新连接并注册到 epoll（略）
        } else {
            // 处理数据或关闭连接（略）
        }
    }
}
```

<br>

## 创建自己的异步库

### 创建自己的事件队列

rust的异步生态中有人尽皆知的**tokio**，其底层依赖的事件队列框架其实是**mio**，这类似于python中**uvloop**背后的**libuv**，它们本质上都是对系统提供的事件队列接口进行封装，当我们学会如何自己封装，我们就掌握了一些底层机制了，具体来说，我们要掌握：

* 如何基于epoll来创建自己的事件队列

* 为什么我们要创建抽象

#### poll&registry

标题的这两个名词代表着mio中最终要的两类抽象：

```rust
// 在根目录下的poll.rs中,Poll是事件队列本身的抽象
pub struct Poll {
    registry: Registry,
}


// 三个关键接口：新建事件队列，返回注册表，轮询poll


// 注册表
pub struct Registry {
    selector: sys::Selector,
    /// Whether this selector currently has an associated waker.
    #[cfg(all(debug_assertions, not(target_os = "wasi")))]
    has_waker: Arc<AtomicBool>,
}

//那么为何要有这个结构体呢？
//该问题可以在下文的并发支持中获得解答
```

#### ET vsLT

| 特性        | 水平触发 (LT) - 默认           | 边缘触发 (ET) - EPOLLET                          |
| --------- | ------------------------ | -------------------------------------------- |
| **触发条件**  | 只要文件描述符**处于**就绪状态        | 文件描述符状态**从未就绪变为就绪**的那一刻                      |
| **通知次数**  | 只要状态满足，每次epoll_wait都可能通知 | 状态变化时**仅通知一次**                               |
| **数据处理**  | 可以不一次性读/写完所有数据，下次还会通知    | **必须**一次性读/写完所有数据，直到返回错误（EAGAIN/EWOULDBLOCK） |
| **编程复杂度** | 相对简单，不易丢失事件              | 较高，必须正确处理循环读/写，否则可能丢失数据                      |
| **效率**    | 可能有冗余通知，效率稍低             | 通知次数少，效率通常更高                                 |
| **配合模式**  | 可与阻塞或非阻塞I/O配合，但非阻塞更常用    | **必须**与非阻塞I/O配合                              |

#### 你自己的事件队列

这一章的核心目标是**从头开始构建一个基于 kqueue 机制的简单事件队列**，并且其API设计会参考流行的底层I/O库 mio。

主要的编程例子（通常在 main.rs 文件中）会做以下事情：

1. **启动一个辅助服务器**：例子需要一个 delayserver 在后台运行。这个服务器的作用是模拟网络延迟，当你向它发送请求时，它会等待指定的毫秒数后再返回响应。

2. **创建多个网络连接**：程序会创建多个 TcpStream（TCP流），同时连接到本地运行的 delayserver。

3. **设置非阻塞模式**：将每个 TcpStream 设置为非阻塞模式。这是异步I/O的基础，意味着读写操作不会卡住线程。

4. **创建并使用自定义事件队列**：
   
   - 初始化在 poll.rs 中定义的 Poll 结构体实例。这个 Poll 结构体封装了底层的 epoll 实例（通过 epoll_create 创建）。
   
   - 对于每一个 TcpStream，程序会通过 Poll 实例中的 Registry（注册表）来注册对读事件（Read events）的兴趣。这通常会调用底层的 epoll_ctl 函数，将流的文件描述符（file descriptor）添加到 epoll 实例的监听列表中。
   
   - 注册时，会关联一个**唯一的令牌（Token）**，通常是一个数字（比如循环的索引 i），用来在之后事件发生时识别是哪个流准备好了。
   
   - 书中提到，例子会使用边缘触发(Edge-Triggered, ET)模式（通过 EPOLLET 标志），这是 mio 的常用模式，也更接近硬件中断的处理方式。

5. **发送HTTP请求**：向每个 TcpStream 写入一个简单的 HTTP GET 请求，请求中会包含不同的延迟时间和标识符。

6. **进入事件循环**：
   
   - 程序进入一个主循环。
   
   - 在循环中，调用 Poll::poll() 方法（内部调用 epoll_wait）。这个调用会**阻塞**当前线程，直到至少有一个已注册的流发生了我们感兴趣的事件（这里是可读事件），或者超时（例子中可能设置为永不超时或一个很长的时间）。**这是关键的等待点**。
   
   - 当 Poll::poll() 返回时，它会带回一个包含就绪事件的列表。
   
   - 程序遍历这些事件。
   
   - 通过事件中包含的**令牌（Token）**，程序能准确地知道是哪个 TcpStream 准备好了数据。
   
   - 从对应的 TcpStream 中读取数据（HTTP响应）。由于使用了边缘触发，这里需要循环读取，直到读完所有可用数据或遇到 WouldBlock 错误，以确保不会错过后续数据（因为边缘触发只通知一次状态变化）。
   
   - 处理（比如打印）读取到的响应。
   
   - 程序会持续这个循环，直到所有请求都收到了响应。

**这个例子最精妙/最有启发性的地方**

这个例子之所以精妙和有启发性，主要在于它**揭示了现代高性能异步I/O的核心工作机制**，并且是**通过自己动手实现**来达成的：

1. **从抽象到底层**：它不是直接使用高级的异步框架（如 Tokio 或 async-std），而是**直接深入到操作系统提供的底层机制** (epoll)。通过 FFI (Foreign Function Interface) 调用 C 库函数 (libc) 中的 epoll 相关系统调用，让你真切地看到异步运行时与操作系统是如何交互的。这打破了异步编程的“魔法”感。

2. **单线程处理高并发的基石**：这个例子（很可能）只使用**单个线程**就管理了多个并发的网络连接和I/O操作。它清晰地展示了**事件驱动**模型：线程只在真正有事可做（I/O 事件就绪）时才工作，在等待期间通过 epoll_wait 将自己阻塞挂起，**让出CPU**，而不是空转轮询。这是 Node.js、Nginx 以及 Rust 中 Tokio 等高性能框架能够用较少线程处理大量并发连接的根本原因。

3. **具体化了“事件循环”**：我们常听说“事件循环”，这个例子就构建了一个最基础的事件循环。Poll::poll() 的阻塞等待和返回后处理就绪事件的循环，就是事件循环的核心骨架。它将抽象的概念落实到了具体的代码和系统调用上。

4. **暴露了底层细节和挑战**：为了让 epoll 正确工作，例子中必须处理一些底层细节，比如：
   
   - 将套接字设为**非阻塞**。
   
   - 理解并正确处理**边缘触发**（需要循环读取直到 WouldBlock）。
   
   - 使用令牌（Token）来关联事件和具体的I/O源。  
     这些都是在使用底层API时必须面对的问题，理解它们有助于更好地使用上层抽象。

5. **连接了 mio 和 epoll**：通过模仿 mio 的 API (Poll, Registry, Token, Interest) 来封装 epoll，这个例子让你不仅学会了 epoll，还大致理解了像 mio 这样的底层I/O库是如何对不同平台的事件通知机制（epoll, kqueue, IOCP）进行**抽象和封装**的，虽然这一章只实现了 epoll 部分。

**总结来说，这个例子最精妙之处在于它不是教你“怎么用异步”，而是通过让你亲手“构建异步的核心引擎（事件队列）”，揭示了“异步为什么能高效工作”的底层原理。它将操作系统、系统调用、并发模型和编程实践结合起来，提供了一个理解高性能网络编程基础的绝佳切入点。** 完成这个例子后，你会对异步运行时（Reactor 部分）的工作方式有更深刻的理解。

<br>

#### 并发支持

1. **分离“等待”和“注册”操作的生命周期和所有权需求**：
   
   - **等待事件 (Poll::poll)** 这个操作通常只应该在**一个**专门的线程（事件循环线程）中进行。这个线程负责驱动整个事件循环，阻塞等待 OS 的通知。因此，Poll 实例通常由这个事件循环线程**拥有**或独占访问。poll() 方法需要 &mut self（可变借用）是很自然的，因为它会接收并可能处理内核返回的事件数据。
   
   - **注册/注销事件 (Registry::register/deregister)** 这个操作可能需要从**多个不同的线程**发起。想象一个多线程的 Web 服务器，一个新的连接可能在任何一个工作线程中被接受（accept），然后这个工作线程需要将这个新的连接注册到共享的事件队列中。

2. **线程安全**：
   
   - 如果只有一个结构体，并且 poll() 需要 &mut self，那么其他线程就无法在 poll() 阻塞期间调用 register() 或 deregister()（因为它们也需要访问同一个结构体，可能也需要 &mut self 或者至少 &self，但可变借用期间不允许其他借用）。
   
   - 通过将注册功能分离到 Registry 中，mio 可以让 Registry **可以被安全地克隆 (try_clone) 并在多个线程之间共享**（通常包裹在 Arc 中）。Registry 内部只需要包含事件队列的句柄副本（一个整数 fd 或 kq），以及可能需要的线程安全机制（如内部锁，尽管在 mio 的某些实现中，注册操作本身可能是线程安全的系统调用）。
   
   - 这样，事件循环线程可以持有 Poll 实例并调用 poll(&mut self) 进行阻塞等待，而其他工作线程可以持有 Registry 的克隆副本或 Arc<Registry>，并在**任何时候**调用 register() 或 deregister() 来管理它们各自的 I/O 资源。这两个操作可以并发进行（或者至少看起来是并发的，底层系统调用可能仍需同步）。

3. **API 设计的清晰性**：
   
   - 将职责分开使得 API 更加清晰。Poll 专注于“等待和接收事件通知”，而 Registry 专注于“管理对事件源的兴趣”。这符合单一职责原则。

**mio 的 try_clone**

Registry::try_clone 这个方法。这正是实现上述多线程注册的关键：

- 事件循环线程创建 Poll 实例。

- 事件循环线程通过 poll.registry() 获取一个 &Registry 引用。

- 这个 &Registry 可以通过 try_clone() 创建出**拥有独立所有权**的 Registry 实例副本。

- 这些独立的 Registry 副本可以被发送到其他线程（或者包裹在 Arc 中共享）。

- 其他线程使用它们自己的 Registry 副本进行注册/注销操作，而事件循环线程继续使用原始的 Poll 实例进行 poll() 等待。

下面我们来直接看看当前最新版本的mio是如何实现的吧

```rust
    pub fn try_clone(&self) -> io::Result<Registry> {
        self.selector.try_clone().map(|selector| Registry {
            selector,
            #[cfg(all(debug_assertions, not(target_os = "wasi")))]
            has_waker: Arc::clone(&self.has_waker),
        })
    }
```

当你调用try_clone，它首先会调用其内部 `selector` 字段的 `try_clone()` 方法。这里的 `self.selector` 是 `sys::Selector` 类型，在 kqueue 环境下，它实际上是 `src/sys/unix/selector/kqueue.rs` 中定义的 `Selector` 类型。

```rust
// Inside Selector::try_clone in src/sys/unix/selector/kqueue.rs
pub fn try_clone(&self) -> io::Result<Selector> {
    self.kq.try_clone().map(|kq| Selector {
        // It's the same selector, so we use the same id.
        #[cfg(debug_assertions)]
        id: self.id,
        kq, // The cloned OwnedFd
    })
}
```

- `kqueue::Selector` 结构体包含一个 `kq: OwnedFd` 字段。这个 `OwnedFd` 是一个封装了文件描述符（File Descriptor, fd）的类型，这个 fd 指向的是通过 `kqueue()` 系统调用创建的内核 kqueue 实例。
- 这里的关键是调用 `self.kq.try_clone()`。`OwnedFd` 是标准库 `std::os::fd` 提供的类型，它的 `try_clone()` 方法会执行 **`dup()` 系统调用**。
- **`dup()` 系统调用**的作用是：创建一个**新的文件描述符**，这个新的文件描述符与旧的文件描述符指向**同一个**底层的内核文件表项（file table entry）。在这个场景下，这个底层内核对象就是那个特定的 kqueue 实例。
- 因此，`self.kq.try_clone()` 成功后会返回一个新的 `OwnedFd`，它包含了一个**新的 fd**，但这个新 fd 和旧 fd 都指向**同一个内核 kqueue 实例**。
- 最后，`kqueue::Selector::try_clone()` 用这个新克隆的 `OwnedFd` 创建并返回一个新的 `kqueue::Selector` 实例。注意，调试用的 `id` 保持不变，表示它们逻辑上是同一个 selector。

<br>

### 实现goroutine

#### ABI含义

- **ABI** 定义了**二进制级别**的接口。它规定了编译后的机器代码如何交互，包括：
  
  - **数据类型的大小、布局和对齐方式：** 比如 int 类型占多少字节，结构体成员在内存中如何排列，内存地址需要按多少字节对齐。
  
  - **函数调用约定 (Calling Convention):** 这是 ABI 的核心部分。它规定了：
    
    - 函数参数如何传递（是通过寄存器还是通过栈？如果是寄存器，用哪些？如果是栈，参数的入栈顺序是什么？）。
    
    - 返回值如何传递（通过哪个寄存器？）。
    
    - 调用函数前后，哪些 CPU 寄存器的值必须由**调用者 (caller)** 保存和恢复，哪些必须由**被调用者 (callee)** 保存和恢复（这就是所谓的 caller-saved 和 callee-saved 寄存器）。
    
    - 函数调用时栈如何设置和清理。
  
  - **系统调用的方式：** 如何从用户程序陷入内核执行系统调用。
  
  - **目标文件的格式、程序库的格式等等。**

简单来说，ABI 确保了由**不同编译器**编译（甚至用不同语言编写，只要它们遵循相同的 ABI）的目标文件、库文件和可执行文件能够**正确地链接和运行**在一起。没有统一的 ABI，A 编译器编译的库就无法被 B 编译器编译的程序调用，因为它们可能对函数参数如何传递、寄存器如何使用等基本问题有不同的“约定”。

<br>

#### Rust内联汇编宏

我们使用了x86-64的cpu架构和systemV，intel汇编。

<br>

#### 栈跳转

```rust
use core::arch::asm; // 导入内联汇编宏

const SSIZE: isize = 48; // 定义一个非常小的栈大小（仅用于演示）

// 代表 CPU 状态的结构体（极简版，只关心栈指针）
#[derive(Debug, Default)]
#[repr(C)] // 保证内存布局与 C 语言兼容
struct ThreadContext {
    rsp: u64, // rsp 寄存器是 x86-64 的栈指针
}

// 我们想在新栈上执行的目标函数
fn hello() -> ! { // "->" 表示这个函数永不返回
    println!("I LOVE WAKING UP ON A NEW STACK!");
    loop {} // 永远循环，模拟任务执行
}

// 实现“切换”到新栈的函数（核心部分）
unsafe fn gt_switch(new: *const ThreadContext) {
    asm!(
        // 将 new 指针指向的 ThreadContext 结构体中的 rsp 成员的值，
        // 移动到 CPU 的 rsp 寄存器中。
        // [{0} + 0x00] 的意思是：取第 0 个输入参数（new 指针）的值，
        // 加上 0 的偏移量（也就是不加偏移），解引用得到内存地址，
        // 再从这个内存地址加载 8 字节（因为 rsp 是 u64）到 rsp 寄存器。
        // 简单说就是： mov rsp, [new]
        "mov rsp, [{0} + 0x00]",
        // 执行 ret 指令。
        "ret",
        // 将 new (指向 ThreadContext 的指针) 作为输入，
        // 让编译器选择一个通用寄存器（reg）来存放它，这个寄存器对应模板中的 {0}。
        in(reg) new,
        // 告诉编译器我们可能修改了内存，并且这个汇编块不会返回（noreturn）。
        // options(nostack) 也可以考虑，明确告知不依赖当前栈。
        options(noreturn)
    );
}

fn main() {
    // 创建一个默认的 ThreadContext 实例
    let mut ctx = ThreadContext::default();
    // 在当前（main 函数）栈上分配一块内存作为“新栈”
    // 这里用 Vec<u8> 模拟，大小为 SSIZE (48字节)
    let mut stack = vec![0_u8; SSIZE as usize];

    unsafe {
        // 计算栈底地址。Vec 的指针指向起始（低地址），栈向下增长，所以栈底是高地址。
        let stack_bottom = stack.as_mut_ptr().offset(SSIZE);
        // 对栈底地址进行 16 字节对齐（System V ABI 要求）。
        // (addr & !15) 是一种常用的向下对齐到 16 倍数地址的技巧。
        let sb_aligned = (stack_bottom as usize & !15) as *mut u8;

        // 在对齐后的栈底向下偏移 16 字节的位置，写入 hello 函数的地址。
        // -16 的位置是 CPU 执行 ret 指令时期望找到返回地址的地方。
        // 我们把 hello 函数的地址伪装成“返回地址”放在这里。
        std::ptr::write(sb_aligned.offset(-16) as *mut u64, hello as u64);

        // 将这个伪造的“返回地址”所在的地址（即新栈的栈顶）
        // 保存到 ctx 的 rsp 字段中。
        ctx.rsp = sb_aligned.offset(-16) as u64;

        // 调用切换函数，传入包含新栈顶指针的 ctx
        gt_switch(&ctx);
        // gt_switch 设置了 noreturn，所以理论上代码不会执行到这里。
    }
}
```

- **ThreadContext 的极简化：** 这里只定义了 rsp 字段。因为这个最简单的例子只关心如何设置**新栈的指针**，暂时不关心保存旧栈的状态。完整纤程切换需要保存更多寄存器。#[repr(C)] 确保 rsp 字段就在结构体的开头，方便汇编代码通过 [{0} + 0x00] 访问。

- **模拟新栈 (Vec<u8>):** 作者用 Vec<u8> 在当前函数（main）的栈（或堆，取决于 Vec 的大小和分配策略，但概念上是在当前执行流的内存空间）中分配了一块连续内存，来**模拟**一个独立的栈空间。

- **栈地址和对齐：**
  
  - 计算 stack_bottom：获取 Vec 内存的末尾地址（高地址）。
  
  - 计算 sb_aligned：因为 System V ABI 要求栈指针（rsp）在函数调用（特别是 call 指令之后，ret 指令之前）必须是 16 字节对齐的，所以作者必须确保我们即将设置给 rsp 的地址满足这个对齐要求。(addr & !15) 是一个位运算技巧，将地址强制向下对齐到最近的 16 的倍数。!15 的二进制是 ...11110000，与地址进行按位与操作会清除低 4 位，从而实现 16 字节对齐。

- **在“新栈”上伪造“返回地址”:** 这是这个例子最核心的技巧。
  
  - std::ptr::write(sb_aligned.offset(-16) as *mut u64, hello as u64);
  
  - 我们知道栈是向下增长的。sb_aligned 是对齐后的栈底（最高有效地址）。
  
  - sb_aligned.offset(-16) 指向的是从对齐栈底向下（向低地址）移动 16 个字节的位置。
  
  - hello as u64 将 hello 函数的地址转换成一个 64 位整数。
  
  - 这行代码的作用就是，在我们准备好的新栈内存的**特定位置**（栈顶-16字节处）写入 hello 函数的地址。
  
  - **为什么是 -16 字节？** 因为 ret 指令会从 rsp 指向的地址**弹出**一个 8 字节的值（在 x86-64 中，地址是 8 字节）作为**返回地址**，然后跳转到这个地址。并且，在 ret 执行**之前**，rsp 通常指向的是栈上的最后一个值（比如 push 的最后一个参数或保存的寄存器），而**返回地址**位于 rsp + 8 的位置（对于标准 call 指令压栈的情况）。但这里我们是**直接设置 rsp**，然后立刻 ret。所以，我们需要让 rsp 直接指向我们存放 hello 函数地址的那个 8 字节内存。同时考虑到 16 字节对齐要求，以及我们可能需要在 hello 函数地址之上（更高地址）预留空间（例如 red zone，虽然这个简单例子没用），将 hello 地址放在 -16 的位置，并将 rsp 指向它，是一个相对安全且符合 ABI 潜在要求的做法。最关键的是，当 ret 执行时，rsp 必须指向存放 hello 地址的地方。

- **设置 ctx.rsp:**
  
  - ctx.rsp = sb_aligned.offset(-16) as u64;
  
  - 我们将刚才存放 hello 函数地址的**那个内存地址**（也就是我们希望 rsp 最终指向的位置）保存到 ctx 结构体的 rsp 字段中。

- **gt_switch 函数的核心逻辑：**
  
  - mov rsp, [{0} + 0x00]：这行汇编代码读取 ctx.rsp 的值（我们刚刚存入的、指向新栈上 hello 函数地址的那个地址），并将其写入 CPU 的 rsp 寄存器。**执行完这句后，CPU 的栈指针就指向了我们新栈上的特定位置！**
  
  - ret：CPU 执行 ret 指令。它会：
    
    1. 从当前 rsp 指向的内存地址（也就是我们新栈上存放 hello 地址的地方）读取 8 字节。
    
    2. 将这 8 字节的值（也就是 hello 函数的地址）加载到**指令指针寄存器 (rip)** 中。
    
    3. 增加 rsp 的值（弹出操作）。
  
  - **结果：** rip 现在指向了 hello 函数的第一条指令。CPU 的下一个指令周期就会去执行 hello 函数的代码。并且，由于 rsp 已经被设置为指向我们的新栈，hello 函数内部的所有栈操作（比如 println! 可能需要的栈空间，虽然这个例子简单，但原理如此）都会发生在我们手动创建的 stack (Vec) 上，而不是原来的 main 函数的栈上。

- **unsafe 关键字：** 整个操作涉及直接读写内存指针 (std::ptr::write) 和执行内联汇编 (asm!) 来修改 CPU 核心寄存器 (rsp) 以及控制流 (ret 技巧)，这些都是 Rust 无法保证内存安全的，所以必须在 unsafe 块中进行。

如果仍然理解不了上述代码，可以在"switch"前加入下面代码来可视化栈空间：

```rust
for i in 0..SSIZE {
 println!("mem: {}, val: {}",
 sb_aligned.offset(-i as isize) as usize,
 *sb_aligned.offset(-i as isize))
}
```

<br>

#### 用户态线程/纤程的实现

下面说说要实现一个用户态线程，我们可能可以想象到的内容：

1. **创建多个纤程：** 每个纤程有自己的栈和执行上下文。

2. **保存和恢复上下文：** 在纤程之间切换时，能够完整地保存当前纤程的执行状态（必要的 CPU 寄存器），并恢复下一个纤程的状态。

3. **调度：** 实现一个（虽然非常简单的）调度器，决定下一个应该运行哪个纤程。

4. **Yield (让出)机制：** 提供一种方式让当前运行的纤程主动放弃 CPU 控制权，让调度器选择其他纤程运行。

5. **纤程生命周期管理：** 处理纤程的创建、运行、就绪、结束等状态。

结合栈跳转的例子，先构思一下该如何实现

<br>

#### generator crate



#### May赏析

1. **这是一个什么项目？** `may` 是一个用于 **Rust 语言的栈式协程库 (Stackful Coroutine Library)**。从 `src/lib.rs` 的文档注释和 `Cargo.toml` 的描述来看，它的目标是提供一个类似于 Go 语言 Goroutine 的并发编程模型，让开发者能够更容易地编写和维护大规模并发程序。

2. **它有什么样的作用？** `may` 库的主要作用是提供一套高效、易用的并发原语和运行时环境，其核心特性包括：
   
   - **栈式协程:** 基于 `generator` crate 实现，协程拥有自己的栈，可以在任意函数调用点挂起和恢复，简化异步逻辑编写。
   - **调度器:** 支持将协程调度到可配置数量的线程上执行，利用多核 CPU 提高性能。支持本地队列和全局队列，并可选地启用工作窃取 (Work Stealing) 策略来平衡负载 (`scheduler.rs`, `crossbeam_queue_shim.rs`)。
   - **协程本地存储 (CLS):** 提供了类似线程本地存储 (TLS) 的机制，但作用域是协程级别 (`local.rs`, `coroutine_local!` 宏)。
   - **异步 I/O:** 集成了高效的异步网络 I/O (如 TCP, UDP) 和可能的其他 I/O 操作，与协程模型无缝结合 (`src/io/`, `src/net/`)。
   - **定时器:** 提供了高效的定时器管理，用于实现 `sleep` 或其他需要超时的操作 (`timeout_list.rs`, `sleep.rs`)。
   - **同步原语:** 提供了协程版本的标准同步原语，如 MPSC/SPMC 队列 (`may_queue/`), 互斥锁, 条件变量, 信号量等 (`src/sync/`)。
   - **协程取消:** 支持取消正在运行或挂起的协程 (`cancel.rs`)。
   - **Panic 处理:** 优雅地处理协程内部的 panic，避免影响其他协程。
   - **作用域协程:** 支持创建保证在特定作用域结束前完成的协程 (`scoped.rs`)。
   - **通用 Select:** 提供了 `select!` 宏，可以同时等待多个不同的异步操作完成 (`cqueue.rs`, `macros.rs`)。

3. **代码实现中最精妙的部份** 这个项目有很多设计精巧的地方，要选出“最”精妙的部分比较主观，但我认为以下几个方面特别值得关注：
   
   - **高效的定时器管理 (`src/timeout_list.rs`)**: `TimeOutList` 的实现非常巧妙。它没有使用简单的线性列表或单一的优先队列来管理所有定时器，而是结合了 **哈希表 (按时间间隔分组)** 和 **最小堆 (管理不同间隔列表的最近到期时间)**。每个时间间隔对应一个独立的 MPSC 队列 (`TimeoutQueueWrapper`)。这种设计：
     
     - **减少了锁竞争:** 不同间隔的定时器操作主要在各自的队列上进行。
     - **提高了查找效率:** 通过哈希表快速定位到特定间隔的队列。
     - **优化了近期事件处理:** 最小堆使得获取下一个最近到期的事件非常高效。
     - 这种分层和分组的设计在处理大量、不同周期的定时事件时，能显著提升性能和扩展性。
   
   - **灵活的调度器与工作窃取 (`src/scheduler.rs`)**: 调度器是协程库的核心。`may` 的调度器设计考虑了多核环境下的性能：
     
     - **混合队列:** 每个工作线程有自己的本地队列 (SPSC 或 SPMC)，同时还有全局队列 (MPSC) 用于跨线程调度。
     - **可选的工作窃取:** 当启用 `work_steal` 特性时，空闲的线程可以从其他线程的本地队列中“窃取”任务来执行，这是一种成熟且高效的负载均衡策略，能有效提高 CPU 利用率。`crossbeam_queue_shim.rs` 或 `may_queue::spmc` 提供了实现工作窃取所需的数据结构。
   
   - **通用的 `cqueue` 和 `select!` 宏 (`src/cqueue.rs`, `src/macros.rs`)**: 不同于 Go 语言主要基于 channel 的 select，`may` 的 `select!` 宏更加通用。它通过 `Cqueue` (Coroutine Queue) 来实现：
     
     - `select!` 的每个分支被包装成一个独立的“选择协程”。
     - 这些选择协程执行其“上半部分”逻辑（例如发起一个异步操作）。
     - 当操作可能阻塞时，选择协程通过 `EventSender::send` 发送一个事件到 `Cqueue` 并挂起自己。
     - 调用 `cqueue.poll` 的主协程会等待 `Cqueue` 中的事件。
     - 一旦收到事件，`poll` 会恢复对应的选择协程，让它执行“下半部分”逻辑。
     - 这种机制允许在任意异步操作（只要能包装成 `FnOnce(EventSender)`）上进行 select，非常灵活和强大。
   
   - **栈式协程与 `generator` 的结合 (`src/coroutine_impl.rs`)**: 虽然底层的 `generator` 是外部库，但 `may` 将其与调度、本地存储、取消、JoinHandle 等机制无缝集成，构建了一个完整且易用的栈式协程运行时，这是整个库的基础，也是其区别于 `async/await` 的核心所在。

<br>

## 代码案例

### python

下列是一段获取用户的数据库模型（sqlmodel）实例的代码片段

```python
db_url = os.getenv("DATABASE_URL", "sqlite:///./local.db")

async_engine = create_async_engine(
    db_url.replace('sqlite:///', 'sqlite+aiosqlite:///'),
    echo=False,
    connect_args={"check_same_thread": False}
)

from app.utils.auth import get_current_user
async with AsyncSession(async_engine) as session:
    user = await get_current_user(token, session)
```
