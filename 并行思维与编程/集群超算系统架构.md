# 集群超算系统架构
记录自己感兴趣的

## 计算架构总览
### 三大定律和三堵墙
* 摩尔定律: CMOS电路的晶体管数每24个月翻番
* Dennard Scaling定律：晶体管越来越小，但功率密度保持不变，因此功率的使用与面积成比例，导致功率和散热问题无法解决
* Amdahl定律：程序中串行代码无法通过核数进行增长

三堵墙
* 算力墙：
    * 工艺升级慢
    * 处理器DIE SIZE极限
    * 功耗不能无限增加
* 内存墙：内存带宽增长无法匹配算力增长（采用多核架构的AMD存在更严峻的挑战）
* 互联墙：PCIe总线时延/带宽无法满足系统互联需求
    * CCIX stack
    * CXL stack
    * nvlink

### 关于互联
这里提到一些最新互联技术的发展，当前许多智算需求（大规模异构计算集群）推动了高性能互联的发展

首先讲下历史：
* 2000年：IBTA成立，试图替代PCIe
* 2014年：以太网发布会100G标准，与IB差距缩小
* 2016年：Intel成立CXL互联
* 2019年：NVIDIA垄断IB
* 2023年：链路带宽网络 >= DDR5（通过IO来扩展内存成为一个选项）

当前数据中心交换网络进入400GE/800GE时代

#### 集群环境中的通信软件栈对比
大家知道现在的AI模型，特别是NLP大模型的参数量超级大，普通的异构服务器已经无法支持这类模型的训练任务，于是我们常说的智算场景应运而生，简单地理解就是异构服务器集群+一些交换节点，换句话所，使用HPC的集群架构以及互联协议，搭载AI的计算芯片。现在我们来对比，在集群环境下最常使用到的两种软件栈：
* RPC: tensorflow就使用的grpc
* MPI: 典型超算场景的标配

我们来对比这两者：
* RPC在众多通讯模型中都位于比较高的层次（注意这里指的是API的层次），MPI在发送数据之前，需要将数据转换为POD，再发送，而rpc要求用户在发消息之前定义好数据的layout，然后通过IDL来发送，rpc框架在发送前会将数据进行压缩/编码
* mpi容灾能力差
* 

### 算力需求

#### 大模型的软硬协同
* 能耗：全球能源供应增长平缓：每年提升2%；但是AI系统2-3年翻一翻（训练算力需求每6-10月翻一番）
* 算力：按2023年中国算力大会新闻发布会的估计，同时处理14亿人的推理请求，需要10^24 FLOPS的计算量，超中国数据中心总算力的3个数量级


### 计算事务分类（算力分类）
* 通用计算：整形为主，单精/双精浮点为辅助
* 科学计算：双精浮点为主，整形为辅
* 概率计算（AI计算）：半精度浮点做训练，INT8做推理
* 物理计算：单精度浮点
