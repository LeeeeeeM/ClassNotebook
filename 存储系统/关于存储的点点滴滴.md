# 我的存储笔记
## 存储介质，产品

### 块设备
比如HDD，SSD，它们：
* 允许访问固定块的数据
* 固定块的数据被称为`blocks`：Block是由文件系统定义的最小的可寻址单元（大多数为4kb），见后文VFS部分


## 存储理论
我们将从一次linux IO按照从前端到后端的顺序进行分解来看一下一个iO完成所需要经过的完整链路，按照如下顺序：
* syscall -> vfs(重点讲block-based fs)
* 

### 文件系统的实现
首先理论方面的学习，主要是先抽象出一个极简的文件系统,再看看现实中的案例。

#### 数据结构
文件系统在磁盘上使用什么类型的结构来组织数据和元数据
简单的文件系统可以是这样：
![file](../statics/Screenshot_20230807_171910_Edge.jpg)

上图为一小块磁盘区域的数据内容，其中：
* 磁盘被分为64个小块(block)，每块大小为4KB
* "D"代表数据，为用户数据的代表
* "I"代表inode，为存放文件元数据的结构
* "d"代表数据位图和inode位图，是一种用于管理空闲块的数据结构
* "i"代表inode位图，用于管理inode块的数据结构
* "S"代表超级块，用于存储文件系统本身的信息

放大0-7数据块(管理结构)的内容：
![inode](../statics/Screenshot_20230807_173332_Edge.jpg)
假设说，我现在希望获取inode32的数据，通过计算得知我需要获得第20KB位（inode大小为256字节）的数据 -> 文件系统要索引这个字节位的数据。但是磁盘是由可寻址扇区组成的(固态硬盘使用闪存芯片存储数据，闪存芯片被分为多个可寻址的扇区，每个扇区都有唯一地址)，通常扇区是512字节，从而文件系统需要向40发出一个读取请求。

除了纯粹数据外的用户数据统称为元数据，设计inode时，最重要的决定之一是它如何引用数据块的位置，一个简单的方法是，将磁盘地址直接写入到inode中，使用这种方法有一个局限性是，对于很大的文件，将没有足够的空间来纳入这些地址。为了解决这类问题，人们想到了**多级索引**

分配一个块专门存其他块的地址，INODE设置12个（举例）直接指针和一个间接指针，一个块可以存1024个指针，则文件上限上涨至4144KB。想要继续扩容，则继续套娃。许多文件系统使用套娃（多级索引）的方法来满足文件大小上限的需求，比如ext2,ext3。

### 块I/O层
linux kernel有一个块IO层是块设备上层的“块抽象”，它像一层夹心饼干，在设备驱动和文件系统之间，文件系统对块设备的访问落到block layer就会变成一次'requests'。
大体上长这样：
![io-resolve](../statics/io-resolve.jpg)

我们看到块IO层其中一个作用是**找到哪里去访问以及拷贝，并确定IOSIZE**，我们得提一下page cache这个概念，即页缓存，首先声明一点：页缓存最重要的作用便是提高文件系统的性能，当应用程序读取文件（或者说使用read系统调用）时，正常的调用链，OS会去查看页缓存中是否存在page cache中了，如果有就直接从当中读取了，所以理所应当地，page cache会存在比磁盘访问性能更高的内存中，下面给出一些命令来让你更深切地感受到page cache的存在。

```shell
# 该命令可以查看到当前linux系统中，页缓存的size
cat /proc/meminfo | grep "Cached:"
```

关于块IO过程中的细节，可以看下文的`一次IO的代价`，这里我们讨论下，下发到驱动前，BIO对象中的"request"所包含的内容，我们先提出一些重要的问题：
* 我们知道文件层的io相关的信息：inode相关信息，按照request这个词本身的意义，我们想知道request会落到哪个设备
* 对于每个设备，一定会有多个`queue`，我们如何知道request落到哪个queue
* 细致考虑到执行过程，我们会想知道：这个req应该是由哪个cpu逻辑核来执行

带着这些问题，我们利用一些现成的工具来帮助我们回溯这个过程：即blktrace


### IO的应用层面分析

#### 一次IO的代价
首先我们看下linux下一次IO需要经过的路径：
![io-trace](../statics/Linux-storage-stack-diagram_v4.0.png)

我们分析一次read操作的执行序列：
![vfs-read](../statics/vfs-read.jpg)

而对于bio对象，则是这样,bio会携带vfs带来的关于IO块的信息，用于落到驱动去查询对应的数据，查到数据后会将数据放到page cache中（非direct iO的情况下）：
![block-read](../statics/block-read.jpg)

而对于Direct IO过程，请看图示：
![direct-io](../statics/direct-io.jpg)

#### 通信层面-json和msgpack

#### 访问方法

#### 存储特征和存储应用设计


## 存储软件

### 存储追踪工具：blktrace
`blktrace`是块IO层级的追踪工具，当你要了解每个下发到块设备的IO操作时，blktrace是最合适的工具，该工具还服务于IO领域内的内核开发者
对比一下,blktrace和iostat：iostat提供了获取向目标设备的请求队列获取信息的能力，但并不覆盖每一个IO

blktrace是很多工具的基础：ioprof, seekwatcher, iowatcher，首先分析一下，一个IO请求，从应用层到块设备，路径如下：
![io-trace](../statics/Linux-storage-stack-diagram_v4.0.png)

blktrace重点分析的过程是block layer中的过程，因此对于block-io过程想要挖掘细节也可以参考这一部分。
我们考虑一个IO进入到block layer，会经历的过程：
* remap：io被device mapper（DM）或者multiple device（MD）映射到其他设备
* split：IO请求可能会因为扇区边界未对齐，size太大而被拆分成多个物理IO
* merge：

### 有哪些衡量指标
我们结合IO理论部分考虑这些可能的指标：
* 性能
    * 大IO带宽极限
    * 小IO带宽极限
    * IO延迟
* 能耗

### 有哪些测试基准
我们先从日常使用、企业级场景、压测这几个维度出发，去考虑三个场景下对于IO任务的一些需求，我们很难将所有场景覆盖完整，因为这个领域（数据存储）所覆盖的内容实在是过于庞大，因此我们从最常见的场景出发来考虑。

#### 个人用户的日常使用
* windows场景、linux场景都需要考虑
* 移动设备的测试需求
* 


#### 个人测试工具


#### 企业使用的存储设备
企业市场一定是相对个人来说更加高端的场景（虽然像废话），我们考虑下对于企业来说，数据存储可能需要具备的特点：
* 高容错
* 容灾
* 超低时延
* 高容忍

### 